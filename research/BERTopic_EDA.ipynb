{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'd:\\code\\ClimateScholar\\wvenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install nbformat>=4.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopicNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached bertopic-0.12.0-py2.py3-none-any.whl (90 kB)\n",
      "Requirement already satisfied: scikit-learn in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (1.0.2)\n",
      "Collecting pandas>=1.1.5\n",
      "  Using cached pandas-1.3.5-cp37-cp37m-win_amd64.whl (10.0 MB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from bertopic) (1.21.6)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from bertopic) (4.64.1)\n",
      "Collecting pyyaml<6.0\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-win_amd64.whl (210 kB)\n",
      "Collecting hdbscan>=0.8.28\n",
      "  Using cached hdbscan-0.8.28.tar.gz (5.2 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting sentence-transformers>=0.4.1\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "Collecting umap-learn>=0.5.0\n",
      "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
      "Collecting plotly>=4.7.0\n",
      "  Downloading plotly-5.10.0-py2.py3-none-any.whl (15.2 MB)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2022.2.1-py2.py3-none-any.whl (500 kB)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from tqdm>=4.41.1->bertopic) (0.4.5)\n",
      "Collecting cython>=0.27\n",
      "  Using cached Cython-0.29.32-py2.py3-none-any.whl (986 kB)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.12.1-cp37-cp37m-win_amd64.whl (161.9 MB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.13.1-cp37-cp37m-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: nltk in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (3.7)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp37-cp37m-win_amd64.whl (1.1 MB)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
      "Collecting numba>=0.49\n",
      "  Downloading numba-0.56.2-cp37-cp37m-win_amd64.whl (2.5 MB)\n",
      "Collecting pynndescent>=0.5\n",
      "  Downloading pynndescent-0.5.7.tar.gz (1.1 MB)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Using cached tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.9.13)\n",
      "Requirement already satisfied: requests in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.28.1)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (4.12.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (4.1.1)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading Pillow-9.2.0-cp37-cp37m-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: click in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.1.3)\n",
      "Requirement already satisfied: setuptools<60 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (47.1.0)\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Downloading llvmlite-0.39.1-cp37-cp37m-win_amd64.whl (23.2 MB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.9.14)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.8.1)\n",
      "Using legacy setup.py install for sentence-transformers, since package 'wheel' is not installed.\n",
      "Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.\n",
      "Using legacy setup.py install for pynndescent, since package 'wheel' is not installed.\n",
      "Building wheels for collected packages: hdbscan\n",
      "  Building wheel for hdbscan (PEP 517): started\n",
      "  Building wheel for hdbscan (PEP 517): finished with status 'done'\n",
      "  Created wheel for hdbscan: filename=hdbscan-0.8.28-cp37-cp37m-win_amd64.whl size=594786 sha256=6774f1f3756e1fd044f977972675f0363d9cf3a49da1ace0d22c07eb7999f78c\n",
      "  Stored in directory: c:\\users\\anthonyhevia\\appdata\\local\\pip\\cache\\wheels\\6e\\7a\\5e\\259ccc841c085fc41b99ef4a71e896b62f5161f2bc8a14c97a\n",
      "Successfully built hdbscan\n",
      "Installing collected packages: pytz, pandas, pyyaml, cython, hdbscan, tokenizers, filelock, huggingface-hub, transformers, torch, pillow, torchvision, sentencepiece, sentence-transformers, llvmlite, numba, pynndescent, umap-learn, tenacity, plotly, bertopic\n",
      "    Running setup.py install for sentence-transformers: started\n",
      "    Running setup.py install for sentence-transformers: finished with status 'done'\n",
      "    Running setup.py install for pynndescent: started\n",
      "    Running setup.py install for pynndescent: finished with status 'done'\n",
      "    Running setup.py install for umap-learn: started\n",
      "    Running setup.py install for umap-learn: finished with status 'done'\n",
      "Successfully installed bertopic-0.12.0 cython-0.29.32 filelock-3.8.0 hdbscan-0.8.28 huggingface-hub-0.9.1 llvmlite-0.39.1 numba-0.56.2 pandas-1.3.5 pillow-9.2.0 plotly-5.10.0 pynndescent-0.5.7 pytz-2022.2.1 pyyaml-5.4.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 tenacity-8.0.1 tokenizers-0.12.1 torch-1.12.1 torchvision-0.13.1 transformers-4.22.1 umap-learn-0.5.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'd:\\code\\ClimateScholar\\wvenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install bertopic scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (4.22.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from transformers) (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: filelock in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: requests in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from transformers) (4.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in d:\\code\\climatescholar\\wvenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'd:\\code\\ClimateScholar\\wvenv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hdbscan in c:\\users\\anthonyhevia\\desktop\\code\\climatescholar\\venv\\lib\\site-packages (0.8.28)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\anthonyhevia\\desktop\\code\\climatescholar\\venv\\lib\\site-packages (from hdbscan) (1.9.3)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\anthonyhevia\\desktop\\code\\climatescholar\\venv\\lib\\site-packages (from hdbscan) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in c:\\users\\anthonyhevia\\desktop\\code\\climatescholar\\venv\\lib\\site-packages (from hdbscan) (1.1.2)\n",
      "Requirement already satisfied: cython>=0.27 in c:\\users\\anthonyhevia\\desktop\\code\\climatescholar\\venv\\lib\\site-packages (from hdbscan) (0.29.28)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\anthonyhevia\\desktop\\code\\climatescholar\\venv\\lib\\site-packages (from hdbscan) (1.23.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\anthonyhevia\\desktop\\code\\climatescholar\\venv\\lib\\site-packages (from scikit-learn>=0.20->hdbscan) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\anthonyhevia\\Desktop\\code\\ClimateScholar\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/scikit-learn-contrib/hdbscan\n",
      "  Cloning https://github.com/scikit-learn-contrib/hdbscan to c:\\users\\anthonyhevia\\appdata\\local\\temp\\pip-req-build-8ztfyhci\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\anthonyhevia\\desktop\\code\\climatescholar\\venv\\lib\\site-packages (from hdbscan==0.8.28) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\anthonyhevia\\desktop\\code\\climatescholar\\venv\\lib\\site-packages (from hdbscan==0.8.28) (1.23.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in c:\\users\\anthonyhevia\\desktop\\code\\climatescholar\\venv\\lib\\site-packages (from hdbscan==0.8.28) (1.1.2)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\anthonyhevia\\desktop\\code\\climatescholar\\venv\\lib\\site-packages (from hdbscan==0.8.28) (1.2.0)\n",
      "Requirement already satisfied: cython>=0.27 in c:\\users\\anthonyhevia\\desktop\\code\\climatescholar\\venv\\lib\\site-packages (from hdbscan==0.8.28) (0.29.28)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\anthonyhevia\\desktop\\code\\climatescholar\\venv\\lib\\site-packages (from scikit-learn>=0.20->hdbscan==0.8.28) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/scikit-learn-contrib/hdbscan 'C:\\Users\\anthonyhevia\\AppData\\Local\\Temp\\pip-req-build-8ztfyhci'\n",
      "WARNING: You are using pip version 21.1.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\anthonyhevia\\Desktop\\code\\ClimateScholar\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/scikit-learn-contrib/hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting joblib==1.1.0\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Installing collected packages: joblib\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.2.0\n",
      "    Uninstalling joblib-1.2.0:\n",
      "      Successfully uninstalled joblib-1.2.0\n",
      "Successfully installed joblib-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\anthonyhevia\\Desktop\\code\\ClimateScholar\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade joblib==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\ClimateScholar\\wvenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving 19 files to the new cache system\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:17<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = []\n",
    "root_path = \"./data/sample\"\n",
    "sample_data = [\"rockfish.jsonl\", \"arctic.jsonl\", \"climate.jsonl\", \"shark_climate.jsonl\"]\n",
    "\n",
    "for data_path in sample_data:\n",
    "    with open(f'{root_path}/{data_path}', 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    result = json.loads(json_list[0])\n",
    "\n",
    "    for result_dict in result[\"data\"]:\n",
    "        abstracts.append(result_dict['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [y for y in (x for x in abstracts) if y is not None]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]\n",
      "2022-09-20 10:18:23,759 - BERTopic - Transformed documents to Embeddings\n",
      "2022-09-20 10:18:32,328 - BERTopic - Reduced dimensionality\n",
      "2022-09-20 10:18:32,344 - BERTopic - Clustered reduced embeddings\n"
     ]
    }
   ],
   "source": [
    "# we add this to remove stopwords\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
    "\n",
    "model = BERTopic(\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    language='english', calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "topics, probs = model.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1_exports_iceland_products_modernisation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0_climate_arctic_ice_sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>1_species_sharks_shark_change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>2_rockfish_fish_dietary_protein</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                       Name\n",
       "0     -1      1  -1_exports_iceland_products_modernisation\n",
       "1      0     50                   0_climate_arctic_ice_sea\n",
       "2      1     42              1_species_sharks_shark_change\n",
       "3      2     32            2_rockfish_fish_dietary_protein"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = model.get_topic_info()\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'climate_arctic_ice_sea_sea ice_global_change_climate change_ocean_atmospheric'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'_'.join([x[0] for x in model.get_topic(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 0, 1, 2]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq.loc[:,\"Topic\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(freq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "745c367861b30f1c58cafb364187217d1090c2a4237d47b4ad449d5ad8917489"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
