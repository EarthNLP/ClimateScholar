{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\climatescholar\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\color\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import hashlib\n",
    "from spacy import Language, util\n",
    "from typing import List\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span\n",
    "from transformers import pipeline\n",
    "import crosslingual_coreference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_wiki_api(item):\n",
    "  try:\n",
    "    url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={item}&language=en&format=json\"\n",
    "    data = requests.get(url).json()\n",
    "    # Return the first id (Could upgrade this in the future)\n",
    "    return data['search'][0]['id']\n",
    "  except:\n",
    "    return 'id-less'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rebel component https://github.com/Babelscape/rebel/blob/main/spacy_component.py\n",
    "def extract_triplets(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    parses the text to triplets\n",
    "    1. Split the text into tokens\n",
    "    2. If the token is <triplet>, <subj>, or <obj>, then set the current variable to the appropriate value\n",
    "    3. If the token is not one of the above, then append it to the appropriate variable\n",
    "    4. If the current variable is <subj>, then append the triplet to the list of triplets\n",
    "    :param text: str - the text to be parsed\n",
    "    :type text: str\n",
    "    :return: A list of dictionaries.\n",
    "    \"\"\"\n",
    "\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = \"\", \"\", \"\", \"\"\n",
    "    text = text.strip()\n",
    "    current = \"x\"\n",
    "\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "\n",
    "        if (token == \"<triplet>\"):\n",
    "\n",
    "            current = \"t\"\n",
    "\n",
    "            if (relation != \"\"):\n",
    "\n",
    "                triplets.append(\n",
    "                        {\n",
    "                            \"head\": subject.strip(),\n",
    "                            \"type\": relation.strip(),\n",
    "                            \"tail\": object_.strip()\n",
    "                            }\n",
    "                        )\n",
    "                relation = \"\"\n",
    "\n",
    "            subject = \"\"\n",
    "\n",
    "        elif (token == \"<subj>\"):\n",
    "\n",
    "            current = \"s\"\n",
    "\n",
    "            if (relation != \"\"):\n",
    "\n",
    "                triplets.append(\n",
    "                    {\n",
    "                        \"head\": subject.strip(),\n",
    "                        \"type\": relation.strip(),\n",
    "                        \"tail\": object_.strip()\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            object_ = \"\"\n",
    "\n",
    "        elif (token == \"<obj>\"):\n",
    "\n",
    "            current = \"o\"\n",
    "            relation = \"\"\n",
    "\n",
    "        else:\n",
    "\n",
    "            if (current == \"t\"):\n",
    "\n",
    "                subject += \" \" + token\n",
    "\n",
    "            elif (current == \"s\"):\n",
    "\n",
    "                object_ += \" \" + token\n",
    "\n",
    "            elif (current == \"o\"):\n",
    "\n",
    "                relation += \" \" + token\n",
    "\n",
    "    if ((subject != \"\") and (relation != \"\") and (object_ != \"\")):\n",
    "\n",
    "        triplets.append(\n",
    "                {\n",
    "                    \"head\": subject.strip(),\n",
    "                    \"type\": relation.strip(),\n",
    "                    \"tail\": object_.strip()\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return triplets\n",
    "\n",
    "\n",
    "@Language.factory(\n",
    "        \"rebel\",\n",
    "        requires = [\"doc.sents\"],\n",
    "        assigns = [\"doc._.rel\"],\n",
    "        default_config = {\n",
    "            \"model_name\": \"Babelscape/rebel-large\",\n",
    "            \"device\": 0,\n",
    "            },\n",
    "        )\n",
    "class RebelComponent:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            nlp, name,\n",
    "            model_name: str,\n",
    "            device: int,\n",
    "        ):\n",
    "\n",
    "        assert model_name is not None, \"\"\n",
    "\n",
    "        self.triplet_extractor = pipeline(\n",
    "                \"text2text-generation\",\n",
    "                model = model_name,\n",
    "                tokenizer = model_name,\n",
    "                device = device\n",
    "                )\n",
    "\n",
    "        # Register custom extension on the Doc\n",
    "        if (not Doc.has_extension(\"rel\")):\n",
    "\n",
    "            Doc.set_extension(\"rel\", default = {})\n",
    "\n",
    "    def _generate_triplets(self, sents: List[Span]) -> List[List[dict]]:\n",
    "        \"\"\"\n",
    "        1. We pass the text of the sentence to the triplet extractor.\n",
    "        2. The triplet extractor returns a list of dictionaries.\n",
    "        3. We extract the token ids from the dictionaries.\n",
    "        4. We decode the token ids into text.\n",
    "        5. We extract the triplets from the text.\n",
    "        6. We return the triplets.\n",
    "        The triplet extractor is a model that takes a sentence as input and returns a list of dictionaries.\n",
    "        Each dictionary contains the token ids of the extracted triplets.\n",
    "        The token ids are the numbers that represent the words in the sentence.\n",
    "        For example, the token id of the word \"the\" is 2.\n",
    "        The token ids are decoded into text using the tokenizer.\n",
    "        The tokenizer is a model that takes a list of token ids as input and returns a list of words.\n",
    "        :param sents: List[Span]\n",
    "        :type sents: List[Span]\n",
    "        :return: A list of lists of dicts.\n",
    "        \"\"\"\n",
    "\n",
    "        output_ids = self.triplet_extractor(\n",
    "                [sent.text for sent in sents],\n",
    "                return_tensors = True,\n",
    "                return_text = False\n",
    "                )  # [0][\"generated_token_ids\"]\n",
    "        extracted_texts = self.triplet_extractor.tokenizer.batch_decode(\n",
    "            [out[\"generated_token_ids\"] for out in output_ids]\n",
    "            )\n",
    "        extracted_triplets = []\n",
    "\n",
    "        for text in extracted_texts:\n",
    "\n",
    "            extracted_triplets.extend(extract_triplets(text))\n",
    "\n",
    "        return extracted_triplets\n",
    "\n",
    "    def set_annotations(self, doc: Doc, triplets: List[dict]):\n",
    "        \"\"\"\n",
    "        The function takes a spacy Doc object and a list of triplets (dictionaries) as input.\n",
    "        For each triplet, it finds the substring in the Doc object that matches the head and tail of the triplet.\n",
    "        It then creates a spacy span object for each of the head and tail.\n",
    "        Finally, it creates a dictionary of the relation type, head span and tail span and adds it to the Doc object\n",
    "        :param doc: the spacy Doc object\n",
    "        :type doc: Doc\n",
    "        :param triplets: List[dict]\n",
    "        :type triplets: List[dict]\n",
    "        \"\"\"\n",
    "\n",
    "        text = doc.text.lower()\n",
    "\n",
    "        for triplet in triplets:\n",
    "\n",
    "            if (triplet[\"head\"] == triplet[\"tail\"]):\n",
    "\n",
    "                continue\n",
    "\n",
    "            head_match = re.search(\n",
    "                r'\\b' + re.escape(triplet[\"head\"].lower()) + r'\\b', text)\n",
    "            if head_match:\n",
    "                head_index = head_match.start()\n",
    "            else:\n",
    "                head_index = text.find(triplet[\"head\"].lower())\n",
    "\n",
    "            tail_match = re.search(\n",
    "                r'\\b' + re.escape(triplet[\"tail\"].lower()) + r'\\b', text)\n",
    "            if tail_match:\n",
    "                tail_index = tail_match.start()\n",
    "            else:\n",
    "                tail_index = text.find(triplet[\"tail\"].lower())\n",
    "\n",
    "            if ((head_index == -1) or (tail_index == -1)):\n",
    "\n",
    "                continue\n",
    "\n",
    "            head_span = doc.char_span(head_index, head_index + len(triplet[\"head\"]), alignment_mode = \"expand\")\n",
    "            tail_span = doc.char_span(tail_index, tail_index + len(triplet[\"tail\"]), alignment_mode = \"expand\")\n",
    "\n",
    "            try:\n",
    "\n",
    "                offset = (head_span.start, tail_span.start)\n",
    "\n",
    "            except (AttributeError):\n",
    "\n",
    "                continue\n",
    "\n",
    "            if (offset not in doc._.rel):\n",
    "\n",
    "                doc._.rel[offset] = {\n",
    "                        \"relation\": triplet[\"type\"],\n",
    "                        \"head_span\": head_span,\n",
    "                        \"tail_span\": tail_span,\n",
    "                        }\n",
    "\n",
    "    def __call__(self, doc: Doc) -> Doc:\n",
    "        \"\"\"\n",
    "        The function takes a doc object and returns a doc object\n",
    "        :param doc: Doc\n",
    "        :type doc: Doc\n",
    "        :return: A Doc object with the sentence triplets added as annotations.\n",
    "        \"\"\"\n",
    "\n",
    "        sentence_triplets = self._generate_triplets(doc.sents)\n",
    "        self.set_annotations(doc, sentence_triplets)\n",
    "\n",
    "        return doc\n",
    "\n",
    "    def pipe(self, stream, batch_size = 128):\n",
    "        \"\"\"\n",
    "        It takes a stream of documents, and for each document,\n",
    "        it generates a list of sentence triplets,\n",
    "        and then sets the annotations for each sentence in the document\n",
    "        :param stream: a generator of Doc objects\n",
    "        :param batch_size: The number of documents to process at a time, defaults to 128 (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        for docs in util.minibatch(stream, size=batch_size):\n",
    "\n",
    "            sents = []\n",
    "\n",
    "            for doc in docs:\n",
    "\n",
    "                sents += doc.sents\n",
    "\n",
    "            sentence_triplets = self._generate_triplets(sents)\n",
    "            index = 0\n",
    "\n",
    "            for doc in docs:\n",
    "\n",
    "                n_sent = len(list(doc.sents))\n",
    "                self.set_annotations(doc, sentence_triplets[index : index + n_sent])\n",
    "                index += n_sent\n",
    "\n",
    "                yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = -1 # Number of the GPU, -1 if want to use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\color\\AppData\\Local\\Temp\\tmp_wecyc2f\\config.json as plain json\n",
      "Some weights of the model checkpoint at nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\code\\climatescholar\\venv\\lib\\site-packages\\allennlp\\models\\archival.py:325: UserWarning: The model models/crosslingual-coreference/minilm/model.tar.gz was trained on a newer version of AllenNLP (v2.9.3), but you're using version 2.10.1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<crosslingual_coreference.CrossLingualPredictorSpacy.CrossLingualPredictorSpacy at 0x2223e9a9b80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add coreference resolution model\n",
    "coref = spacy.load('en_core_web_sm', disable=['ner', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'])\n",
    "coref.add_pipe(\n",
    "    \"xx_coref\", config={\"chunk_size\": 2500, \"chunk_overlap\": 2, \"device\": DEVICE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RebelComponent at 0x222781e58b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define rel extraction model\n",
    "rel_ext = spacy.load('en_core_web_sm', disable=['ner', 'lemmatizer', 'attribute_rules', 'tagger'])\n",
    "rel_ext.add_pipe(\"rebel\", config={\n",
    "    'device':DEVICE, # Number of the GPU, -1 if want to use CPU\n",
    "    'model_name':'Babelscape/rebel-large'} # Model used, will default to 'Babelscape/rebel-large' if not given\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The idea behind the marine cloud-brightening (MCB) geoengineering technique is that seeding marine stratocumulus clouds with copious quantities of roughly monodisperse sub-micrometre sea water particles might significantly enhance the cloud droplet number concentration, and thereby the cloud albedo and possibly longevity.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"The idea behind the marine cloud-brightening (MCB) geoengineering technique is that seeding marine stratocumulus clouds with copious quantities of roughly monodisperse sub-micrometre sea water particles might significantly enhance the cloud droplet number concentration, and thereby the cloud albedo and possibly longevity.\"\n",
    "\n",
    "coref_text = coref(input_text)._.resolved_text\n",
    "print(coref_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 11): {'relation': 'instance of', 'head_span': marine cloud-brightening, 'tail_span': geoengineering}\n"
     ]
    }
   ],
   "source": [
    "doc = rel_ext(coref_text)\n",
    "\n",
    "for value, rel_dict in doc._.rel.items():\n",
    "    print(f\"{value}: {rel_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1eabbc9a406c53db59c49ebe6f40286eabb20418c59c68041ab7ce3464042c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
