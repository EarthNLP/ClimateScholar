{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anthonyhevia\\Desktop\\code\\ClimateScholar\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import pke\n",
    "import json\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "import os\n",
    "import pickle\n",
    "import concise_concepts\n",
    "from SentenceGraph.core import SentenceGraph, Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Types\n",
    "#### Paper Node\n",
    "- Title\n",
    "- paperId\n",
    "- Full Abstract\n",
    "#### Entity Node\n",
    "- entity_name\n",
    "#### Author Node\n",
    "- authorId\n",
    "- Name\n",
    "#### Sentence Node\n",
    "- Sentence text\n",
    "- Sentence Embedding\n",
    "\n",
    "### Topic Node\n",
    "- name\n",
    "\n",
    "### Edge Types\n",
    "- (Sentence)-[:PREVIOUS_SENTENCE]->Sentence\n",
    "- (Sentence)-[:NEXT_SENTENCE]->Sentence\n",
    "- (Paper)-[:AUTHORED]->Author\n",
    "- (Paper)-[:HAS_KEYWORD]->Keyphrase\n",
    "- (Keyphrase)-[:RELATION]->Keyphrase\n",
    "- (Paper)-[:IN_TOPIC]->Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePickle(data, save_path) -> None:\n",
    "    try:\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error: {e} with trying to save pickle at: {save_path}\")\n",
    "\n",
    "\n",
    "def loadPickle(load_path) -> None:\n",
    "    try:\n",
    "        with open(load_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error: {e} with trying to load pickle at: {load_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts = []\n",
    "root_path = \"./data/sample\"\n",
    "sample_data = [\"weather_CO2.jsonl\", \"paleoclimate.jsonl\", \"rewilding.jsonl\", \"rockfish.jsonl\", \"arctic.jsonl\", \"climate.jsonl\", \"shark_climate.jsonl\"]\n",
    "\n",
    "for data_path in sample_data:\n",
    "    with open(f'{root_path}/{data_path}', 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    result = json.loads(json_list[0])\n",
    "\n",
    "    for result_dict in result[\"data\"]:\n",
    "        abstracts.append(result_dict)\n",
    "\n",
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [y for y in (x for x in abstracts) if y[\"abstract\"] is not None]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 7/7 [00:20<00:00,  2.96s/it]\n",
      "2022-12-16 15:13:28,775 - BERTopic - Transformed documents to Embeddings\n",
      "2022-12-16 15:13:40,101 - BERTopic - Reduced dimensionality\n",
      "2022-12-16 15:13:40,123 - BERTopic - Clustered reduced embeddings\n"
     ]
    }
   ],
   "source": [
    "# we add this to remove stopwords\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
    "\n",
    "model = BERTopic(\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    language='english', calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "topics, probs = model.fit_transform([x['abstract'] for x in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize keyphrase extraction model, here TopicRank\n",
    "extractor = pke.unsupervised.TopicRank()\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ent_dict = {}\n",
    "# This loop is the main processing loop\n",
    "for item, topicId in zip(data, topics):\n",
    "    topicNormalized = '_'.join([x[0] for x in model.get_topic(topicId)])\n",
    "    item['topic'] = topicNormalized\n",
    "\n",
    "    abstract = item['abstract']\n",
    "    entities = []\n",
    "\n",
    "    if abstract is None:\n",
    "        print(\"Found a none!\")\n",
    "    else:\n",
    "        # Extract entities\n",
    "        extractor.load_document(input=abstract, language='en')\n",
    "        extractor.candidate_selection()\n",
    "        extractor.candidate_weighting()\n",
    "        ents_keep = [x[0] for x in extractor.get_n_best(n=2)]\n",
    "        item['ents'] = ents_keep\n",
    "\n",
    "        try:\n",
    "            topic_ent_dict[topicNormalized].extend(ents_keep)\n",
    "        except KeyError:\n",
    "            topic_ent_dict[topicNormalized] = ents_keep\n",
    "\n",
    "        # Tokenize sentences\n",
    "        doc = nlp(item['abstract'])\n",
    "        item['sentences'] = [sent for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePickle(topic_ent_dict, './data/topic_ent_dict_checkpoint.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_labels = topic_ent_dict.keys()\n",
    "for topic in entity_labels:\n",
    "    topic_ent_dict[topic] = list(set(topic_ent_dict[topic]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePickle(data, './data/data_checkpoint.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"hackathon\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populating the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paper_node(tx, paperId: str, title: str, abstract: str) -> None:\n",
    "    tx.run(\"CREATE (a:Paper {paperId: $paperId, title: $title, abstract: $abstract})\", paperId=paperId, title=title, abstract=abstract)\n",
    "\n",
    "def create_entity_node(tx, entity_name: str, label_name: str) -> None:\n",
    "    tx.run(\"CREATE (a:Entity {entity_name: $entity_name, label_name: $label_name})\", entity_name=entity_name, label_name=label_name)\n",
    "\n",
    "def create_topic_node(tx, topic_name: str) -> None:\n",
    "    tx.run(\"CREATE (a:Topic {topic_name: $topic_name})\", topic_name=topic_name)\n",
    "    \n",
    "def create_sentence_node(tx, paperId: str, sentence_id: str, sentence_txt: str) -> None:\n",
    "    tx.run(\"CREATE (s:Sentence {paperId: $paperId, sentence_id: $sentence_id, sentence_txt: $sentence_txt})\", paperId=paperId, sentence_id=sentence_id, sentence_txt=sentence_txt)\n",
    "\n",
    "def create_author_node(tx, authorId: str, author_name: str) -> None:\n",
    "    tx.run(\"CREATE (a:Author {authorId: $authorId, author_name: $author_name})\", authorId=authorId, author_name=author_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_authored_relationship(tx, paperId, authorId):\n",
    "    tx.run(\"MATCH (a:Author) WHERE a.authorId = $authorId \"\n",
    "            \"MATCH (b:Paper) WHERE b.paperId = $paperId \"\n",
    "            \"CREATE (a)-[:AUTHORED]->(b)\",\n",
    "           authorId=authorId, paperId=paperId)\n",
    "\n",
    "def create_entity_relationship(tx, paperId, entity_name):\n",
    "    tx.run(\"MATCH (a:Entity) WHERE a.entity_name = $entity_name \"\n",
    "            \"MATCH (b:Paper) WHERE b.paperId = $paperId \"\n",
    "            \"CREATE (b)-[:HAS_KEYWORD]->(a)\",\n",
    "           entity_name=entity_name, paperId=paperId)\n",
    "\n",
    "def create_in_topic_relationship(tx, paperId, topic_name):\n",
    "    tx.run(\"MATCH (topic:Topic) WHERE topic.topic_name = $topic_name \"\n",
    "            \"MATCH (paper:Paper) WHERE paper.paperId = $paperId \"\n",
    "            \"CREATE (paper)-[:IN_TOPIC]->(topic)\",\n",
    "           topic_name=topic_name, paperId=paperId)\n",
    "\n",
    "def create_sentence_relationship(tx, sentence_id, paperId):\n",
    "    tx.run(\"MATCH (s:Sentence) WHERE s.sentence_id = $sentence_id \"\n",
    "            \"MATCH (p:Paper) WHERE p.paperId = $paperId \"\n",
    "            \"CREATE (p)-[:HAS_SENTENCE]->(s)\",\n",
    "           sentence_id=sentence_id, paperId=paperId)\n",
    "\n",
    "def create_semantic_sentence_relationship(tx, sentence_id1, sentence_id2, score):\n",
    "    tx.run(\"MATCH (s1:Sentence) WHERE s1.sentence_id = $sentence_id1 \"\n",
    "            \"MATCH (s2:Sentence) WHERE s2.sentence_id = $sentence_id2 \"\n",
    "            \"CREATE (s1)-[:SIMILARITY_SCORE {score: $score}]->(s2)\",\n",
    "           sentence_id1=sentence_id1, sentence_id2=sentence_id2, score=score)\n",
    "\n",
    "## TODO: Many of the above relationships are not bidirectional. We need to create queries that point the other direction as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_authors = set()\n",
    "seen_ents = set()\n",
    "seen_topics = set()\n",
    "\n",
    "with driver.session() as session:\n",
    "    for item in data:\n",
    "        # Create the core paper node\n",
    "        session.execute_write(create_paper_node, item['paperId'], item['title'], item['abstract'])\n",
    "\n",
    "        if item['topic'] not in seen_topics:\n",
    "            seen_topics.add(item['topic'])\n",
    "            session.execute_write(create_topic_node, item['topic'])\n",
    "        \n",
    "        session.execute_write(create_in_topic_relationship, item['paperId'], item['topic'])\n",
    "\n",
    "        # Create the entity nodes\n",
    "        for ent in item['ents']:\n",
    "            if ent not in seen_ents:\n",
    "                seen_ents.add(ent)\n",
    "                label_name = f\"{item['topic']}\"\n",
    "                session.execute_write(create_entity_node, ent, label_name)\n",
    "\n",
    "            session.execute_write(create_entity_relationship, item['paperId'], ent)\n",
    "\n",
    "        # Create the author nodes and relationships\n",
    "        for author in item['authors']:\n",
    "            if author['authorId'] not in seen_authors:\n",
    "                seen_authors.add(author['authorId'])\n",
    "                session.execute_write(create_author_node, author['authorId'], author['name'])\n",
    "\n",
    "            session.execute_write(create_authored_relationship, item['paperId'], author['authorId'])\n",
    "\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create semantic relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_paper_nodes(tx):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_node(tx, sentenceId: int, sentence: str, sentence_embedding) -> None:\n",
    "    tx.run(\"CREATE (s:Sentence {sentenceId: $sentenceId, sentence: $sentence, sentence_embedding: $sentence_embedding})\", sentenceId=sentenceId, sentence=sentence, sentence_embedding=sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_paper_relationship(tx, paperId, sentenceId):\n",
    "    tx.run(\"MATCH (sentence:Sentence) WHERE sentence.sentenceId = $sentenceId \"\n",
    "            \"MATCH (paper:Paper) WHERE paper.paperId = $paperId \"\n",
    "            \"CREATE (sentence)-[:IN_PAPER]->(paper)\"\n",
    "            \"CREATE (paper)-[:HAS_SENTENCE]->(sentence)\",\n",
    "           sentenceId=sentenceId, paperId=paperId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_paper_relationship(tx, paperId, sentenceId):\n",
    "    tx.run(\"MATCH (sentence:Sentence) WHERE sentence.sentenceId = $sentenceId \"\n",
    "            \"MATCH (paper:Paper) WHERE paper.paperId = $paperId \"\n",
    "            \"CREATE (sentence)-[:IN_PAPER]->(paper)\"\n",
    "            \"CREATE (paper)-[:HAS_SENTENCE]->(sentence)\",\n",
    "           sentenceId=sentenceId, paperId=paperId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceGraph = SentenceGraph()\n",
    "simularity_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with driver.session() as session:\n",
    "    # Grab all the paper nodes\n",
    "    paper_nodes = session.read_transaction(get_all_paper_nodes)\n",
    "    \n",
    "    # a bit hacky, but works!\n",
    "    sentences = []\n",
    "    paperIds = []\n",
    "\n",
    "    # Iterate through all the paper nodes create the list of sentences\n",
    "    for paper in paper_nodes:\n",
    "        pass\n",
    "    \n",
    "    # TODO: SentenceGraph does return plain embeddings\n",
    "    sim_graph = sentenceGraph.createGraph(sentences)\n",
    "\n",
    "    # Create sentence nodes & their relationships to papers\n",
    "\n",
    "\n",
    "    # Create edges between semantically similar sentences\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for j, sentence in enumerate(sentences):\n",
    "            if sim_graph[i][j] > simularity_threshold:\n",
    "                # Create the sentence2sentence relationship if they are above a certain simularity \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "745c367861b30f1c58cafb364187217d1090c2a4237d47b4ad449d5ad8917489"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
