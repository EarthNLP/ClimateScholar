{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\climatescholar\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "import pickle\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install --upgrade joblib==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePickle(data, save_path) -> None:\n",
    "    try:\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error: {e} with trying to save pickle at: {save_path}\")\n",
    "\n",
    "\n",
    "def loadPickle(load_path) -> None:\n",
    "    try:\n",
    "        with open(load_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error: {e} with trying to load pickle at: {load_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts = []\n",
    "root_path = \"./data/sample\"\n",
    "sample_data = [\"weather_CO2.jsonl\", \"paleoclimate.jsonl\", \"rewilding.jsonl\", \"rockfish.jsonl\", \"arctic.jsonl\", \"climate.jsonl\", \"shark_climate.jsonl\"]\n",
    "\n",
    "for data_path in sample_data:\n",
    "    with open(f'{root_path}/{data_path}', 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    result = json.loads(json_list[0])\n",
    "\n",
    "    for result_dict in result[\"data\"]:\n",
    "        abstracts.append(result_dict)\n",
    "\n",
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [y for y in (x for x in abstracts) if y[\"abstract\"] is not None]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 7/7 [02:21<00:00, 20.20s/it]\n",
      "2023-03-25 19:48:48,618 - BERTopic - Transformed documents to Embeddings\n",
      "2023-03-25 19:48:57,579 - BERTopic - Reduced dimensionality\n",
      "2023-03-25 19:48:57,775 - BERTopic - Clustered reduced embeddings\n"
     ]
    }
   ],
   "source": [
    "# we add this to remove stopwords\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
    "abstract_encoder = SentenceTransformer(\"sentence-transformers/allenai-specter\")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=abstract_encoder,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    language='english', \n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform([x['abstract'].replace(\"\\n\", \" \").rstrip() for x in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.save(\"models/climate_scholar_specter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "kw_extractor = KeyBERT(model=abstract_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ent_dict = {}\n",
    "# This loop is the main processing loop\n",
    "for item, topicId in zip(data, topics):\n",
    "    topicNormalized = '_'.join([x[0] for x in topic_model.get_topic(topicId)])\n",
    "    item['topic'] = topicNormalized\n",
    "\n",
    "    abstract = item['abstract'].replace(\"\\n\", \" \").rstrip()\n",
    "    entities = []\n",
    "\n",
    "    if abstract is not None:\n",
    "        # Extract keywords\n",
    "        keywords = kw_extractor.extract_keywords(abstract, keyphrase_ngram_range=(1, 2), stop_words='english',use_maxsum=True, nr_candidates=20, top_n=7,  use_mmr=True, diversity=0.5)\n",
    "        item['keywords'] = [x[0].lower() for x in keywords]\n",
    "\n",
    "        try:\n",
    "            topic_ent_dict[topicNormalized].extend(item['keywords'])\n",
    "        except KeyError:\n",
    "            topic_ent_dict[topicNormalized] = item['keywords']\n",
    "        \n",
    "        item['embeddings'] = abstract_encoder.encode(abstract).tolist()\n",
    "\n",
    "        # We could map a paper to multiple topics using sentences!\n",
    "        # TODO: Unsure if we're actually going to do anything regarding sentences. Might be adding too much noise\n",
    "        # Process with spaCy\n",
    "        # doc = nlp(item['abstract'])\n",
    "        # item['sentences'] = [sent.text for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePickle(topic_ent_dict, './data/topic_ent_dict_checkpoint.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"hackathon\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cypher "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Creation queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_paper_node(tx, paperId: str, title: str, abstract: str, embedding) -> None:\n",
    "#     tx.run(\"CREATE (a:Paper {paperId: $paperId, title: $title, abstract: $abstract, embedding: $embedding})\", paperId=paperId, title=title, abstract=abstract, embedding=embedding)\n",
    "\n",
    "def create_paper_node(tx, paperId: str, title: str, abstract: str) -> None:\n",
    "     tx.run(\"CREATE (a:Paper {paperId: $paperId, title: $title, abstract: $abstract})\", paperId=paperId, title=title, abstract=abstract)\n",
    "\n",
    "def create_keyword_node(tx, entity_name: str,) -> None:\n",
    "    tx.run(\"CREATE (a:Keyword {entity_name: $entity_name})\", entity_name=entity_name)\n",
    "\n",
    "def create_topic_node(tx, topic_name: str) -> None:\n",
    "    tx.run(\"CREATE (a:Topic {topic_name: $topic_name})\", topic_name=topic_name)\n",
    "    \n",
    "def create_sentence_node(tx, paperId: str, sentence_id: str, sentence_txt: str) -> None:\n",
    "    tx.run(\"CREATE (s:Sentence {paperId: $paperId, sentence_id: $sentence_id, sentence_txt: $sentence_txt})\", paperId=paperId, sentence_id=sentence_id, sentence_txt=sentence_txt)\n",
    "\n",
    "def create_author_node(tx, authorId: str, author_name: str) -> None:\n",
    "    tx.run(\"CREATE (a:Author {authorId: $authorId, author_name: $author_name})\", authorId=authorId, author_name=author_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword Relationships\n",
    "\n",
    "TODO: Lets start switching over to using Keyword instead of Entity. More user friendly imo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paper_has_keyword_relationship(tx, paperId, entity_name):\n",
    "    tx.run(\"MATCH (kw:Keyword) WHERE kw.entity_name = $entity_name \"\n",
    "            \"MATCH (paper:Paper) WHERE paper.paperId = $paperId \"\n",
    "            \"CREATE (paper)-[:HAS_KEYWORD]->(kw)\",\n",
    "           entity_name=entity_name, paperId=paperId)\n",
    "\n",
    "def create_keyword_in_paper_relationship(tx, paperId, entity_name):\n",
    "    tx.run(\"MATCH (kw:Keyword) WHERE kw.entity_name = $entity_name \"\n",
    "            \"MATCH (paper:Paper) WHERE paper.paperId = $paperId \"\n",
    "            \"CREATE (kw)-[:IN_PAPER]->(paper)\",\n",
    "           entity_name=entity_name, paperId=paperId)\n",
    "\n",
    "def create_keyword_cooccurence_relationship(tx, entity_name1, entity_name2):\n",
    "    tx.run(\"MATCH (kw1:Keyword) WHERE kw1.entity_name = $entity_name1 \"\n",
    "            \"MATCH (kw2:Keyword) WHERE kw2.entity_name = $entity_name2 \"\n",
    "            \"CREATE (kw1)-[:COOCCURS_WITH]->(kw2)\"\n",
    "            \"CREATE (kw2)-[:COOCCURS_WITH]->(kw1)\",\n",
    "           entity_name1=entity_name1, entity_name2=entity_name2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paper_in_topic_relationship(tx, paperId, topic_name):\n",
    "    tx.run(\"MATCH (topic:Topic) WHERE topic.topic_name = $topic_name \"\n",
    "            \"MATCH (paper:Paper) WHERE paper.paperId = $paperId \"\n",
    "            \"CREATE (paper)-[:IN_TOPIC]->(topic)\",\n",
    "           topic_name=topic_name, paperId=paperId)\n",
    "\n",
    "def create_topic_has_paper_relationship(tx, paperId, topic_name):\n",
    "    tx.run(\"MATCH (topic:Topic) WHERE topic.topic_name = $topic_name \"\n",
    "            \"MATCH (paper:Paper) WHERE paper.paperId = $paperId \"\n",
    "            \"CREATE (topic)-[:HAS_PAPER]->(paper)\",\n",
    "           topic_name=topic_name, paperId=paperId)\n",
    "\n",
    "def create_keyword_in_topic_relationship(tx, entity_name, topic_name):\n",
    "    tx.run(\"MATCH (topic:Topic) WHERE topic.topic_name = $topic_name \"\n",
    "            \"MATCH (kw:Keyword) WHERE kw.entity_name = $entity_name \"\n",
    "            \"CREATE (kw)-[:IN_TOPIC]->(topic)\",\n",
    "           topic_name=topic_name, entity_name=entity_name)\n",
    "\n",
    "def create_topic_has_keyword_relationship(tx, entity_name, topic_name):\n",
    "    tx.run(\"MATCH (topic:Topic) WHERE topic.topic_name = $topic_name \"\n",
    "            \"MATCH (kw:Keyword) WHERE kw.entity_name = $entity_name \"\n",
    "            \"CREATE (topic)-[:HAS_KEYWORD]->(kw)\",\n",
    "           topic_name=topic_name, entity_name=entity_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paper_has_sentence_relationship(tx, sentence_id, paperId):\n",
    "    tx.run(\"MATCH (s:Sentence) WHERE s.sentence_id = $sentence_id \"\n",
    "            \"MATCH (p:Paper) WHERE p.paperId = $paperId \"\n",
    "            \"CREATE (p)-[:HAS_SENTENCE]->(s)\",\n",
    "           sentence_id=sentence_id, paperId=paperId)\n",
    "\n",
    "def create_sentence_in_paper_relationship(tx, sentence_id, paperId):\n",
    "    tx.run(\"MATCH (s:Sentence) WHERE s.sentence_id = $sentence_id \"\n",
    "            \"MATCH (p:Paper) WHERE p.paperId = $paperId \"\n",
    "            \"CREATE (s)-[:IN_PAPER]->(p)\",\n",
    "           sentence_id=sentence_id, paperId=paperId)\n",
    "\n",
    "def create_semantic_sentence_relationship(tx, sentence_id1, sentence_id2, score):\n",
    "    tx.run(\"MATCH (s1:Sentence) WHERE s1.sentence_id = $sentence_id1 \"\n",
    "            \"MATCH (s2:Sentence) WHERE s2.sentence_id = $sentence_id2 \"\n",
    "            \"CREATE (s1)-[:SIMILAR {score: $score}]->(s2)\",\n",
    "           sentence_id1=sentence_id1, sentence_id2=sentence_id2, score=score)\n",
    "\n",
    "def create_keyword_in_sentence_relationship(tx, entity_name, sentence_id):\n",
    "    tx.run(\"MATCH (sent:Sentence) WHERE s.sentence_id = $sentence_id \"\n",
    "            \"MATCH (kw:Keyword) WHERE kw.entity_name = $entity_name \"\n",
    "            \"CREATE (kw)-[:IN_SENTENCE]->(sent)\",\n",
    "           sentence_id=sentence_id, entity_name=entity_name)\n",
    "\n",
    "def create_sentence_has_keyword_relationship(tx, entity_name, sentence_id):\n",
    "    tx.run(\"MATCH (sent:Sentence) WHERE s.sentence_id = $sentence_id \"\n",
    "            \"MATCH (kw:Keyword) WHERE kw.entity_name = $entity_name \"\n",
    "            \"CREATE (sent)-[:HAS_KEYWORD]->(kw)\",\n",
    "           sentence_id=sentence_id, entity_name=entity_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_authored_relationship(tx, paperId, authorId):\n",
    "    tx.run(\"MATCH (a:Author) WHERE a.authorId = $authorId \"\n",
    "            \"MATCH (b:Paper) WHERE b.paperId = $paperId \"\n",
    "            \"CREATE (a)-[:AUTHORED]->(b)\"\n",
    "            \"CREATE (b)-[:AUTHORED]->(a)\",\n",
    "           authorId=authorId, paperId=paperId)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populating the graph (from scratch)\n",
    "Run this code only during local development or if youre recreating a graph from scratch. Not meant to touch the production graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_id(paperId: str, count: int) -> str:\n",
    "    f\"{paperId}-{count}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to read from defunct connection IPv4Address(('localhost', 7687)) (IPv4Address(('127.0.0.1', 7687)))\n"
     ]
    },
    {
     "ename": "IncompleteCommit",
     "evalue": "Failed to read from defunct connection IPv4Address(('localhost', 7687)) (IPv4Address(('127.0.0.1', 7687)))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32md:\\code\\climatescholar\\venv\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:53\u001b[0m, in \u001b[0;36mInbox._buffer_one_chunk\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mwhile\u001b[39;00m chunk_size \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     52\u001b[0m     \u001b[39m# Determine the chunk size and skip noop\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     receive_into_buffer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_socket, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer, \u001b[39m2\u001b[39;49m)\n\u001b[0;32m     54\u001b[0m     chunk_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer\u001b[39m.\u001b[39mpop_u16()\n",
      "File \u001b[1;32md:\\code\\climatescholar\\venv\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:293\u001b[0m, in \u001b[0;36mreceive_into_buffer\u001b[1;34m(sock, buffer, n_bytes)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39mwhile\u001b[39;00m buffer\u001b[39m.\u001b[39mused \u001b[39m<\u001b[39m end:\n\u001b[1;32m--> 293\u001b[0m     n \u001b[39m=\u001b[39m sock\u001b[39m.\u001b[39;49mrecv_into(view[buffer\u001b[39m.\u001b[39;49mused:end], end \u001b[39m-\u001b[39;49m buffer\u001b[39m.\u001b[39;49mused)\n\u001b[0;32m    294\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32md:\\code\\climatescholar\\venv\\lib\\site-packages\\neo4j\\_async_compat\\network\\_bolt_socket.py:472\u001b[0m, in \u001b[0;36mBoltSocket.recv_into\u001b[1;34m(self, buffer, nbytes)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrecv_into\u001b[39m(\u001b[39mself\u001b[39m, buffer, nbytes):\n\u001b[1;32m--> 472\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_io(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_socket\u001b[39m.\u001b[39;49mrecv_into, buffer, nbytes)\n",
      "File \u001b[1;32md:\\code\\climatescholar\\venv\\lib\\site-packages\\neo4j\\_async_compat\\network\\_bolt_socket.py:447\u001b[0m, in \u001b[0;36mBoltSocket._wait_for_io\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deadline \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 447\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    448\u001b[0m timeout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_socket\u001b[39m.\u001b[39mgettimeout()\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] An existing connection was forcibly closed by the remote host",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mIncompleteCommit\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [49], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[39mfor\u001b[39;00m ent2 \u001b[39min\u001b[39;00m item[\u001b[39m'\u001b[39m\u001b[39ments\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m     31\u001b[0m         \u001b[39mif\u001b[39;00m ent \u001b[39m!=\u001b[39m ent2:\n\u001b[1;32m---> 32\u001b[0m             session\u001b[39m.\u001b[39mexecute_write(create_keyword_cooccurence_relationship, ent, ent2)\n\u001b[0;32m     34\u001b[0m \u001b[39m# Create the author nodes and relationships\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39mfor\u001b[39;00m author \u001b[39min\u001b[39;00m item[\u001b[39m'\u001b[39m\u001b[39mauthors\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[1;32md:\\code\\climatescholar\\venv\\lib\\site-packages\\neo4j\\_sync\\work\\session.py:672\u001b[0m, in \u001b[0;36mSession.execute_write\u001b[1;34m(self, transaction_function, *args, **kwargs)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexecute_write\u001b[39m(\n\u001b[0;32m    628\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    629\u001b[0m     transaction_function: t\u001b[39m.\u001b[39mCallable[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    632\u001b[0m     \u001b[39m*\u001b[39margs: _P\u001b[39m.\u001b[39margs,  \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: _P\u001b[39m.\u001b[39mkwargs\n\u001b[0;32m    633\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m _R:\n\u001b[0;32m    634\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Execute a unit of work in a managed write transaction.\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \n\u001b[0;32m    636\u001b[0m \u001b[39m    .. note::\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[39m    .. versionadded:: 5.0\u001b[39;00m\n\u001b[0;32m    671\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 672\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_transaction(\n\u001b[0;32m    673\u001b[0m         WRITE_ACCESS, transaction_function, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    674\u001b[0m     )\n",
      "File \u001b[1;32md:\\code\\climatescholar\\venv\\lib\\site-packages\\neo4j\\_sync\\work\\session.py:499\u001b[0m, in \u001b[0;36mSession._run_transaction\u001b[1;34m(self, access_mode, transaction_function, *args, **kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    498\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m         tx\u001b[39m.\u001b[39;49m_commit()\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (DriverError, Neo4jError) \u001b[39mas\u001b[39;00m error:\n\u001b[0;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disconnect()\n",
      "File \u001b[1;32md:\\code\\climatescholar\\venv\\lib\\site-packages\\neo4j\\_sync\\work\\transaction.py:177\u001b[0m, in \u001b[0;36mTransactionBase._commit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39mcommit(on_success\u001b[39m=\u001b[39mmetadata\u001b[39m.\u001b[39mupdate)\n\u001b[0;32m    176\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39msend_all()\n\u001b[1;32m--> 177\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection\u001b[39m.\u001b[39;49mfetch_all()\n\u001b[0;32m    178\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bookmark \u001b[39m=\u001b[39m metadata\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mbookmark\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    179\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_database \u001b[39m=\u001b[39m metadata\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdb\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_database)\n",
      "File \u001b[1;32md:\\code\\climatescholar\\venv\\lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:672\u001b[0m, in \u001b[0;36mBolt.fetch_all\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    670\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponses[\u001b[39m0\u001b[39m]\n\u001b[0;32m    671\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m response\u001b[39m.\u001b[39mcomplete:\n\u001b[1;32m--> 672\u001b[0m     detail_delta, summary_delta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetch_message()\n\u001b[0;32m    673\u001b[0m     detail_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m detail_delta\n\u001b[0;32m    674\u001b[0m     summary_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m summary_delta\n",
      "File \u001b[1;32md:\\code\\climatescholar\\venv\\lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:655\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m    654\u001b[0m \u001b[39m# Receive exactly one message\u001b[39;00m\n\u001b[1;32m--> 655\u001b[0m tag, fields \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minbox\u001b[39m.\u001b[39;49mpop(\n\u001b[0;32m    656\u001b[0m     hydration_hooks\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresponses[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mhydration_hooks\n\u001b[0;32m    657\u001b[0m )\n\u001b[0;32m    658\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_message(tag, fields)\n\u001b[0;32m    659\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39midle_since \u001b[39m=\u001b[39m perf_counter()\n",
      "File \u001b[1;32md:\\code\\climatescholar\\venv\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:74\u001b[0m, in \u001b[0;36mInbox.pop\u001b[1;34m(self, hydration_hooks)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpop\u001b[39m(\u001b[39mself\u001b[39m, hydration_hooks):\n\u001b[1;32m---> 74\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer_one_chunk()\n\u001b[0;32m     75\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         size, tag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unpacker\u001b[39m.\u001b[39munpack_structure_header()\n",
      "File \u001b[1;32md:\\code\\climatescholar\\venv\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:70\u001b[0m, in \u001b[0;36mInbox._buffer_one_chunk\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m (\n\u001b[0;32m     67\u001b[0m     \u001b[39mOSError\u001b[39;00m, SocketDeadlineExceeded, asyncio\u001b[39m.\u001b[39mCancelledError\n\u001b[0;32m     68\u001b[0m ) \u001b[39mas\u001b[39;00m error:\n\u001b[0;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_broken \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     Util\u001b[39m.\u001b[39;49mcallback(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_error, error)\n\u001b[0;32m     71\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32md:\\code\\climatescholar\\venv\\lib\\site-packages\\neo4j\\_async_compat\\util.py:113\u001b[0m, in \u001b[0;36mUtil.callback\u001b[1;34m(cb, *args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcallback\u001b[39m(cb, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m callable(cb):\n\u001b[1;32m--> 113\u001b[0m         \u001b[39mreturn\u001b[39;00m cb(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\code\\climatescholar\\venv\\lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:681\u001b[0m, in \u001b[0;36mBolt._set_defunct_read\u001b[1;34m(self, error, silent)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_defunct_read\u001b[39m(\u001b[39mself\u001b[39m, error\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, silent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    678\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFailed to read from defunct connection \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    679\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munresolved_address, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mserver_info\u001b[39m.\u001b[39maddress\n\u001b[0;32m    680\u001b[0m     )\n\u001b[1;32m--> 681\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_defunct(message, error\u001b[39m=\u001b[39;49merror, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[1;32md:\\code\\climatescholar\\venv\\lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:723\u001b[0m, in \u001b[0;36mBolt._set_defunct\u001b[1;34m(self, message, error, silent)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(response, CommitResponse):\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m error:\n\u001b[1;32m--> 723\u001b[0m         \u001b[39mraise\u001b[39;00m IncompleteCommit(message) \u001b[39mfrom\u001b[39;00m \u001b[39merror\u001b[39;00m\n\u001b[0;32m    724\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    725\u001b[0m         \u001b[39mraise\u001b[39;00m IncompleteCommit(message)\n",
      "\u001b[1;31mIncompleteCommit\u001b[0m: Failed to read from defunct connection IPv4Address(('localhost', 7687)) (IPv4Address(('127.0.0.1', 7687)))"
     ]
    }
   ],
   "source": [
    "seen_authors = set()\n",
    "seen_keywords = set()\n",
    "seen_topics = set()\n",
    "\n",
    "with driver.session() as session:\n",
    "    for item in data:\n",
    "        # Create the core paper node\n",
    "        # session.execute_write(create_paper_node, item['paperId'], item['title'], item['abstract'], item['embeddings'])\n",
    "        session.execute_write(create_paper_node, item['paperId'], item['title'], item['abstract'])\n",
    "\n",
    "\n",
    "        if item['topic'] not in seen_topics:\n",
    "            seen_topics.add(item['topic'])\n",
    "            session.execute_write(create_topic_node, item['topic'])\n",
    "        \n",
    "        session.execute_write(create_paper_in_topic_relationship, item['paperId'], item['topic'])\n",
    "        session.execute_write(create_topic_has_paper_relationship, item['paperId'], item['topic'])\n",
    "\n",
    "        # Create the entity nodes\n",
    "        for ent in item['keywords']:\n",
    "            if ent not in seen_keywords:\n",
    "                seen_keywords.add(ent)\n",
    "                session.execute_write(create_keyword_node, ent)\n",
    "                session.execute_write(create_keyword_in_topic_relationship, ent, item['topic'])\n",
    "                session.execute_write(create_topic_has_keyword_relationship, ent, item['topic'])\n",
    "\n",
    "            session.execute_write(create_paper_has_keyword_relationship, item['paperId'], ent)\n",
    "            session.execute_write(create_keyword_in_paper_relationship, item['paperId'], ent)\n",
    "\n",
    "        # Create the keyword co-occurence relationships\n",
    "        for ent in item['keywords']:\n",
    "            for ent2 in item['keywords']:\n",
    "                if ent != ent2:\n",
    "                    session.execute_write(create_keyword_cooccurence_relationship, ent, ent2)\n",
    "\n",
    "        # Create the author nodes and relationships\n",
    "        for author in item['authors']:\n",
    "            if author['authorId'] not in seen_authors:\n",
    "                seen_authors.add(author['authorId'])\n",
    "                session.execute_write(create_author_node, author['authorId'], author['name'])\n",
    "\n",
    "            session.execute_write(create_authored_relationship, item['paperId'], author['authorId'])\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1eabbc9a406c53db59c49ebe6f40286eabb20418c59c68041ab7ce3464042c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
